# ğŸ¯ UCB Algorithm for Online Ad Click-Through Rate Optimization

This project demonstrates how to use the **UCB (Upper Confidence Bound)** algorithmâ€”a powerful **Reinforcement Learning** 
methodâ€”to optimize **online advertisement selection** with the goal of maximizing **click-through rate (CTR)**.

---
## ğŸ“Š What the Dataset Is Like

The dataset simulates **user interactions** with **10 different online ads**, each designed differently (e.g., SUV on a mountain,
city, moon, countryside, etc.).

### Structure:

* **Rows**: Each row corresponds to a **user interaction (total 10,000 users)**.
* **Columns**: Each column represents a **different ad (Ad 1 to Ad 10)**.
* **Cell values**: Binary (0 or 1)

  * `1` means the user **clicked on the ad**.
  * `0` means the user **did not click on the ad**.

**Important Note**:
This dataset is a **simulation**, not real-time data. Itâ€™s created to imitate real-world user behavior for reinforcement learning 
experiments.

Example (fictional):

| Ad 1 | Ad 2 | Ad 3 | Ad 4 | Ad 5 | Ad 6 | Ad 7 | Ad 8 | Ad 9 | Ad 10 |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ----- |
| 1    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1    | 0     |
| 0    | 1    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0     |
| ...  | ...  | ...  | ...  | ...  | ...  | ...  | ...  | ...  | ...   |

Each user is shown **one ad**, and the UCB algorithm is used to **balance exploration vs exploitation** to determine which ad leads 
to the highest CTR over time.

---

## ğŸ“ Project Structure

```
ğŸ“‚ reinforcement-learning-ucb
â”‚
â”œâ”€â”€ ğŸ“„ ucb_ad_selection.ipynb   # Implementation of the UCB algorithm
â”œâ”€â”€ ğŸ“„ Ads_CTR_Optimisation.csv # Simulated dataset (10,000 users, 10 ads)
â”œâ”€â”€ ğŸ“„ README.md                # Project overview and setup instructions
```

---

## ğŸ§  Problem Statement

A car dealership created **10 different ad designs** for a luxury SUV. The goal is to determine **which ad leads to the most clicks**
from users. Showing all ads equally would be inefficient and costly. This project uses UCB to:

* Dynamically learn from user interactions
* Select the **best performing ad** in **minimum time/rounds**
* Reduce ad impression costs

---

## ğŸ“Š Dataset Description

* **Rows:** 10,000 simulated users
* **Columns:** 10 ads (`Ad_1` to `Ad_10`)
* **Cell Values:** `1` = user clicked, `0` = no click

Each row simulates a user interaction, and the value in each column indicates whether the user would have clicked on the
corresponding ad if shown.

---

## ğŸ“ˆ Algorithm Used

### âœ… Upper Confidence Bound (UCB)

The UCB algorithm is a **multi-armed bandit** strategy that balances:

* **Exploration**: Trying all ads to gather data.
* **Exploitation**: Focusing on the ad(s) with the highest estimated CTR.

#### UCB Steps:

1. For each round (user):

   * Compute UCB for each ad based on past clicks and impressions.
   * Select the ad with the highest UCB.
   * Update statistics based on the observed reward (click or no click).

---

## ğŸ“Š Results

* The UCB algorithm efficiently identifies the ad with the highest CTR.
* A **histogram plot** shows how many times each ad was selected.
* The most frequently selected ad corresponds to the **optimal ad**.

---

## ğŸš€ How to Run

### ğŸ›  Requirements

* Python 3.x
* Jupyter Notebook or Google Colab
* Libraries: `numpy`, `matplotlib`, `pandas`

### â–¶ï¸ Steps

1. Clone this repository
2. Open `ucb_ad_selection.ipynb` in Jupyter or Colab
3. Run the notebook to simulate ad selection using UCB

---

## ğŸ“· Output Visualization

The project ends with a histogram that shows the number of times each ad was selectedâ€”highlighting the best-performing ad over 
10,000 rounds.

---

## ğŸ”® Future Work

* Compare with **Thompson Sampling**
* Add real-time dashboards
* Test with real-world ad campaign data


## Understanding UCB

with AB testing, youâ€™re relying on collecting a significant amount of data before you make any changes. It's a static approach:
you gather your data, analyze the outcomes, and then switch to the best performer. But here's the issueâ€”what if one ad 
is significantly outperforming the others, and youâ€™re still wasting time and money showing the low-performing ones just to 
finish your test?

Thatâ€™s where reinforcement learningâ€”and specifically the multi-armed bandit approachâ€”comes in.
Instead of waiting for all the data to come in, the multi-armed bandit algorithm learns and adapts in real time. As users interact 
with each ad, the algorithm starts favoring the ads that perform better (get more clicks or conversions), while still occasionally
testing the others to ensure it doesnâ€™t miss out on a potentially better one. This smart balance between exploration (trying out
each ad enough to learn) and exploitation (showing the current best ad more often) allows advertisers to maximize returns
while learning.

To summarize:
AB Testing: Show all ads equally until enough data is gathered. Then pick the best. (Good for static evaluation)

Multi-Armed Bandit: Adaptively learn which ad is best while optimizing performance. (Good for real-time improvement)

ğŸ¯ What is UCB?
The Upper Confidence Bound (UCB) algorithm helps decide which machine to choose, by combining:

The empirical mean reward (what weâ€™ve observed so far).

A confidence term that favors machines tried fewer times (more uncertainty = wider bounds = more chance to explore it).

It always picks the arm with the highest upper bound = (mean reward + confidence term).

ğŸ‘¶ Simple Example (Step-by-Step)
Suppose you have 3 ads (Ad A, Ad B, Ad C). The click-through rates (CTR) are:

Ad A: 0.1

Ad B: 0.3

Ad C: 0.7

You donâ€™t know these in advance. The UCB algorithm helps you learn it over time.

ğŸ” Round 1 to 3: Try each ad once
We try each ad once for exploration.

Assume:

Ad A â†’ user did NOT click (reward = 0)

Ad B â†’ user clicked (reward = 1)

Ad C â†’ user clicked (reward = 1)

Current state:

|Ad |Times Shown	| Total Reward |	Mean|
|---|-------------|--------------|------|
| A	|     1	      |     0	       | 0.0  |
| B |    	1	      |     1	       | 1.0  |
| C	|     1	      |     1	       | 1.0  |

Now round = 4, letâ€™s compute UCB:
 
So:

UCB(A) = 0.0 + sqrt(2 * ln(4) / 1) â‰ˆ 1.665

UCB(B) = 1.0 + sqrt(2 * ln(4) / 1) â‰ˆ 2.665

UCB(C) = 1.0 + sqrt(2 * ln(4) / 1) â‰ˆ 2.665

âœ… Choose B or C (they have the same upper bound).

Suppose we choose B again, and this time user didnâ€™t click. So reward = 0.

âš™ï¸ After Round 4:
Updated:

|Ad |Times Shown	| Total Reward |	Mean|
|---|-------------|--------------|------|
| A	|     1	      |     0	       | 0.0  |
| B |    	2	      |     1	       | 0.5  |
| C	|     1	      |     1	       | 1.0  |

Now round = 5.

UCBs:

UCB(A) = 0.0 + sqrt(2 * ln(5) / 1) â‰ˆ 1.794

UCB(B) = 0.5 + sqrt(2 * ln(5) / 2) â‰ˆ 1.543

UCB(C) = 1.0 + sqrt(2 * ln(5) / 1) â‰ˆ 2.794

âœ… Choose C (highest UCB).
