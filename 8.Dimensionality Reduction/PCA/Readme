Used iN
ðŸ”¹ Unsupervised Learning	Because PCA doesn't use labeled data.
ðŸ”¹ Dimensionality Reduction	PCA is one of the most popular techniques here.
ðŸ”¹ Feature Extraction / Selection	PCA helps reduce the number of features while keeping the important patterns.
ðŸ”¹ Data Preprocessing / Preparation	Often used before feeding data into ML models to reduce size and noise.



Imagine you have a big coloring book full of drawings, and each drawing has many colors and shapes. But to understand the main idea 
of the drawing, you donâ€™t need to look at **every** little color or detail â€” just the **important parts**.

PCA is like a smart pair of glasses that helps you **look at only the most important things** in a big mess of data (like shapes and 
colors). It helps you see **what really matters** by shrinking the picture but still keeping the big idea clear.

So if your picture has 10 colors, PCA might help you understand it using only 2 or 3 â€” much simpler, but still useful!

It also lets you **turn and twist the drawing** (like in 3D!) to better see whatâ€™s going on. But be careful â€” if you spill ink
(like an outlier), it might mess up your view.



**Principal Component Analysis (PCA)** is a widely-used **unsupervised dimensionality reduction technique**. Its main objective is to:

* Identify **correlations between features**.
* Find the **directions (principal components) of maximum variance** in high-dimensional data.
* **Project** this data into a lower-dimensional space (from D to K dimensions, where K < D) while preserving as much information as 
possible.

PCA is commonly applied in:

* **Visualization**
* **Feature extraction**
* **Noise filtering**
* Domains like **finance (e.g., stock prediction)** and **bioinformatics (e.g., gene analysis)**.

---

### ðŸ’¡ PCA Algorithm Steps:

1. **Standardize** the dataset.
2. Compute **covariance matrix**.
3. Find **eigenvalues and eigenvectors** of the covariance matrix.
4. Sort eigenvectors by descending eigenvalues.
5. Select top K eigenvectors â†’ form **projection matrix W**.
6. **Transform** the original data using W.

---

### âš ï¸ Important Note:

* **Not predictive** like linear regression â€” PCA is **exploratory**.
* It is **sensitive to outliers** â€” which can distort the principal components.

PCA helps in **understanding structure** in data, especially when visualized in **2D or 3D** to observe how data behaves along the new 
axes of maximum variance.




# ðŸ§ª Wine Customer Segmentation with PCA and Logistic Regression

This project demonstrates **dimensionality reduction** using **Principal Component Analysis (PCA)** on the **Wine dataset**,
followed by training a **logistic regression model** to classify wines into **customer segments**. The aim is to reduce feature
complexity while preserving key variance in the data, ultimately supporting a **recommender system** for wine merchants.

âœ”ï¸ Itâ€™s predicting the type of wine (from 3 categories),
based on its chemical properties using a model trained on labeled data.

Each wine type was originally created from grapes grown in three different cultivars (varieties).

These cultivars resulted in different chemical properties in the wine, and experts labeled them as Class 0, 1, or 2.

Now we can use machine learning to say:

â€œIf a wine has this alcohol, this ash content, this flavonoid level... it probably belongs to Class 1.â€

---

## ðŸ“Œ Project Overview

In a practical business scenario, a **wine merchant** wants to:

1. **Understand customer segments** based on wine features.
2. **Reduce the complexity** of the wine feature data.
3. **Build a predictive model** to recommend wines to the appropriate customer segment.

We achieve this by:

* Reducing dimensionality of the dataset using **PCA**.
* Training a **Logistic Regression** classifier on the reduced data.
* Predicting **customer segments** for new wines based on their features.

---

## ðŸ“‚ Dataset Description

Dataset: [`wine.csv`](https://archive.ics.uci.edu/ml/datasets/Wine)
Source: UCI Machine Learning Repository
Modified: Last column renamed to **Customer Segment** for business relevance.

### ðŸ” Features (Input Variables)

The dataset contains **13 numerical features** describing the chemical properties of different wines:

| Feature                        | Description                 |
| ------------------------------ | --------------------------- |
| `Alcohol`                      | Alcohol content             |
| `Malic_Acid`                   | Amount of malic acid        |
| `Ash`                          | Ash content                 |
| `Alcalinity_of_Ash`            | Alkalinity of ash           |
| `Magnesium`                    | Magnesium level             |
| `Total_Phenols`                | Total phenols present       |
| `Flavanoids`                   | Flavanoid concentration     |
| `Nonflavanoid_Phenols`         | Nonflavonoid phenols        |
| `Proanthocyanins`              | Level of proanthocyanins    |
| `Color_Intensity`              | Color intensity of the wine |
| `Hue`                          | Hue of the wine             |
| `OD280/OD315_of_Diluted_Wines` | OD280/OD315 value           |
| `Proline`                      | Proline level in the wine   |

---

### ðŸŽ¯ Target Variable (Output)

* **Customer Segment** (formerly `Class` in original dataset)
* Represents **3 segments of wine customers**:

  * `0`: Segment A
  * `1`: Segment B
  * `2`: Segment C

Each segment is a group of customers with similar wine preferences.

---

## ðŸ§  Dimensionality Reduction with PCA

* **Objective**: Reduce the feature space from 13 â†’ 2 dimensions.
* **Why**:

  * Reduce computational cost.
  * Improve model interpretability.
  * Retain maximum variance with fewer components.
* **PCA Result**:

  * Two new features: `Principal Component 1`, `Principal Component 2`.
  * These two components capture most of the variance in the dataset.

---

## ðŸ§® Model Pipeline

1. **Import Libraries**
   (`pandas`, `numpy`, `matplotlib`, `sklearn`)

2. **Load & Preprocess Data**

   * Load dataset.
   * Split into training and test sets.
   * Feature scale using `StandardScaler`.

3. **Apply PCA**

   * Reduce to 2 principal components.
   * Transform both training and test sets.

4. **Train Classifier**

   * Model: `LogisticRegression`
   * Train on PCA-reduced training set.

5. **Evaluate Model**

   * Confusion Matrix
   * Visualization of decision boundaries (train and test sets)

---

## ðŸ“ˆ Visualizations

* 2D plots of decision regions post-PCA.
* Shows how well the reduced dimensions separate customer segments.


## ðŸ“¦ Requirements


numpy
pandas
matplotlib
scikit-learn
jupyterlab / colab


