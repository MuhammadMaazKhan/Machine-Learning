Project ########

The project where we apply Grid Search with Cross-Validation to find the best hyperparameters for a Kernel SVM classifier. This 
is based on the Social Network Ads dataset and aims to demonstrate how model performance can be optimized through 
hyperparameter tuning.


Imagine you're trying to bake the best cake. ğŸ°
You try different amounts of sugar, different oven temperatures, and different baking times.

You try every combination:

1 spoon sugar + 150Â°C + 20 minutes

2 spoons sugar + 150Â°C + 20 minutes

1 spoon sugar + 180Â°C + 25 minutes

...and so on.

Then you taste each cake and pick the one thatâ€™s the most yummy!

ğŸ¯ This is like Grid Search:
You try all possible combinations of settings to find the best one.

Grid Search is a technique used in machine learning to find the best hyperparameters for a model.

ğŸ” It does this by:

Defining a grid of possible values (like a menu of choices).

Trying every combination of those values.

Evaluating each one (often using cross-validation).

Picking the one with the best performance.



## âœ… 1. **Why these numbers for SVM? (`C`, `gamma`)**

These numbers are not fixed; they are **chosen to test different model behaviors**.

### ğŸ”¹ `C` (Regularization Parameter)

* Controls **model complexity**:

  * **Low `C` (e.g. 0.01, 0.1)** â†’ simpler model, more tolerant of errors, less overfitting.
  * **High `C` (e.g. 1, 10, 100)** â†’ complex model, fits training data tightly, higher chance of overfitting.

ğŸ§ª Example test values: `[0.01, 0.1, 1, 10, 100]`

---

### ğŸ”¹ `gamma` (RBF kernel only)

* Controls **how far each training example affects the decision boundary**.

  * **Low gamma (e.g. 0.001, 0.01)** â†’ smoother model, generalizes better.
  * **High gamma (e.g. 0.1, 1, 10)** â†’ more flexible model, may overfit.

ğŸ§ª Example test values: `['scale', 'auto', 0.001, 0.01, 0.1, 1, 10]`

---

### ğŸ“Œ Why those numbers?

Because:

* They test a **wide range** (small â†’ big).
* Help find the **best balance** between underfitting and overfitting.
* Not too many values â€” to keep training fast.

---

## âœ… 2. **What values do we use for other models?**

Each model has **its own important hyperparameters** â€” here's a cheat sheet:

---

### ğŸ”¸ **Logistic Regression**

```python
'C': [0.01, 0.1, 1, 10, 100]
'penalty': ['l1', 'l2']
'solver': ['liblinear', 'saga']
```

---

### ğŸ”¸ **K-Nearest Neighbors (KNN)**

```python
'n_neighbors': [3, 5, 7, 9]
'weights': ['uniform', 'distance']
'metric': ['euclidean', 'manhattan']
```

---

### ğŸ”¸ **Decision Tree**

```python
'max_depth': [None, 5, 10, 20]
'min_samples_split': [2, 5, 10]
'criterion': ['gini', 'entropy']
```

---

### ğŸ”¸ **Random Forest**

```python
'n_estimators': [50, 100, 200]
'max_depth': [None, 10, 20]
'min_samples_leaf': [1, 2, 4]
```

---

### ğŸ”¸ **Gradient Boosting / XGBoost**

```python
'n_estimators': [50, 100, 200]
'learning_rate': [0.01, 0.1, 0.2]
'max_depth': [3, 5, 7]
```

---

### ğŸ”¸ **K-Means (Clustering)**

```python
'n_clusters': [2, 3, 4, 5, 6, 7, 8, 9, 10]
'init': ['k-means++', 'random']
'max_iter': [100, 300, 500]
```

---

## ğŸ§  Final Tip

Thereâ€™s no **universal fixed set of values** â€” we try a **range** of small to large numbers to find what works best for
**our specific dataset**.

We use tools like **`GridSearchCV`** or **`RandomizedSearchCV`** to automatically test combinations.


